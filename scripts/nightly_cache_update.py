#!/usr/bin/env python3
"""
å¤œé—´æ•°æ®ç¼“å­˜æ›´æ–°è„šæœ¬
æ¯å¤©å‡Œæ™¨è‡ªåŠ¨æ›´æ–°å¸¸ç”¨çš„é‡‘èæ•°æ®åˆ°æœ¬åœ°ç¼“å­˜
"""

import asyncio
import json
import logging
import sys
import os
from datetime import datetime, timedelta
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ç¡®ä¿å·¥ä½œç›®å½•æ˜¯é¡¹ç›®æ ¹ç›®å½•
os.chdir(project_root)

try:
    from adaptors.akshare import AKShareAdaptor
    from core.mcp_protocol import MCPRequest
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"Pythonè·¯å¾„: {sys.path}")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("nightly_update.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("nightly-cache-update")

class NightlyCacheUpdater:
    def __init__(self, cache_dir="static/cache/system"):
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        cache_path = Path(cache_dir)
        cache_path.mkdir(parents=True, exist_ok=True)

        self.adaptor = AKShareAdaptor(cache_dir=cache_dir)
        self.update_config = self._load_update_config()
        self.cache_dir = cache_path

    def _load_update_config(self):
        """åŠ è½½éœ€è¦æ›´æ–°çš„æ•°æ®é…ç½®"""
        config_file = Path("scripts/nightly_update_config.json")
        try:
            if config_file.exists():
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
                    return config
            else:
                logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")

        # é»˜è®¤é…ç½® - ä½¿ç”¨ç®€å•çš„æµ‹è¯•æ•°æ®
        today = datetime.now()
        start_date = (today - timedelta(days=7)).strftime("%Y%m%d")  # æœ€è¿‘7å¤©
        end_date = today.strftime("%Y%m%d")

        return {
            "daily_updates": [
                {
                    "interface": "index_zh_a_hist",
                    "params": {
                        "symbol": "000001",
                        "period": "daily",
                        "start_date": start_date,
                        "end_date": end_date
                    },
                    "description": "ä¸Šè¯æŒ‡æ•°è¿‘7å¤©æ•°æ®"
                },
                {
                    "interface": "index_zh_a_hist",
                    "params": {
                        "symbol": "000300",
                        "period": "daily",
                        "start_date": start_date,
                        "end_date": end_date
                    },
                    "description": "æ²ªæ·±300æŒ‡æ•°è¿‘7å¤©æ•°æ®"
                }
            ],
            "weekly_updates": [
                {
                    "interface": "stock_zh_a_hist",
                    "params": {
                        "symbol": "000001",
                        "period": "daily",
                        "start_date": (today - timedelta(days=30)).strftime("%Y%m%d"),
                        "end_date": end_date
                    },
                    "description": "å¹³å®‰é“¶è¡Œè¿‘30å¤©æ•°æ®"
                }
            ],
            "cache_settings": {
                "cleanup_days": 30,
                "max_file_size_mb": 50,
                "retry_attempts": 3,
                "retry_delay_seconds": 5
            }
        }
    
    async def update_single_interface(self, interface_config):
        """æ›´æ–°å•ä¸ªæ¥å£æ•°æ®ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        interface = interface_config["interface"]
        params = interface_config["params"]
        description = interface_config.get("description", interface)

        cache_settings = self.update_config.get("cache_settings", {})
        retry_attempts = cache_settings.get("retry_attempts", 3)
        retry_delay = cache_settings.get("retry_delay_seconds", 5)

        for attempt in range(retry_attempts):
            try:
                if attempt > 0:
                    logger.info(f"é‡è¯•ç¬¬ {attempt} æ¬¡: {description}")
                    await asyncio.sleep(retry_delay)
                else:
                    logger.info(f"å¼€å§‹æ›´æ–°: {description} ({interface})")

                # è°ƒç”¨AkShareæ¥å£
                result = await self.adaptor.call(interface, **params)

                if result is not None and not (hasattr(result, 'empty') and result.empty):
                    data_size = len(result) if hasattr(result, '__len__') else 1
                    logger.info(f"âœ… æˆåŠŸæ›´æ–°: {description} (æ•°æ®é‡: {data_size})")
                    return True
                else:
                    logger.warning(f"âš ï¸ æ•°æ®ä¸ºç©º: {description}")
                    if attempt == retry_attempts - 1:
                        return False

            except Exception as e:
                logger.error(f"âŒ æ›´æ–°å¤±è´¥ (å°è¯• {attempt + 1}/{retry_attempts}): {description} - {str(e)}")
                if attempt == retry_attempts - 1:
                    return False

        return False
    
    async def run_daily_updates(self):
        """æ‰§è¡Œæ¯æ—¥æ›´æ–°"""
        logger.info("=== å¼€å§‹æ¯æ—¥æ•°æ®æ›´æ–° ===")
        
        daily_configs = self.update_config.get("daily_updates", [])
        success_count = 0
        total_count = len(daily_configs)
        
        for config in daily_configs:
            success = await self.update_single_interface(config)
            if success:
                success_count += 1
            
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            await asyncio.sleep(2)
        
        logger.info(f"=== æ¯æ—¥æ›´æ–°å®Œæˆ: {success_count}/{total_count} æˆåŠŸ ===")
        return success_count, total_count
    
    async def run_weekly_updates(self):
        """æ‰§è¡Œæ¯å‘¨æ›´æ–°ï¼ˆä»…å‘¨æ—¥æ‰§è¡Œï¼‰"""
        if datetime.now().weekday() != 6:  # 0=Monday, 6=Sunday
            logger.info("ä»Šå¤©ä¸æ˜¯å‘¨æ—¥ï¼Œè·³è¿‡å‘¨æ›´æ–°")
            return 0, 0
            
        logger.info("=== å¼€å§‹æ¯å‘¨æ•°æ®æ›´æ–° ===")
        
        weekly_configs = self.update_config.get("weekly_updates", [])
        success_count = 0
        total_count = len(weekly_configs)
        
        for config in weekly_configs:
            success = await self.update_single_interface(config)
            if success:
                success_count += 1
            
            # å‘¨æ›´æ–°é—´éš”æ›´é•¿
            await asyncio.sleep(5)
        
        logger.info(f"=== æ¯å‘¨æ›´æ–°å®Œæˆ: {success_count}/{total_count} æˆåŠŸ ===")
        return success_count, total_count
    
    async def cleanup_old_cache(self, days_to_keep=30):
        """æ¸…ç†è¿‡æœŸç¼“å­˜æ–‡ä»¶"""
        logger.info(f"=== å¼€å§‹æ¸…ç† {days_to_keep} å¤©å‰çš„ç¼“å­˜æ–‡ä»¶ ===")

        # ä½¿ç”¨å®ä¾‹çš„ç¼“å­˜ç›®å½•
        if not self.cache_dir.exists():
            logger.info("ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†")
            return

        cutoff_time = datetime.now() - timedelta(days=days_to_keep)
        deleted_count = 0

        for cache_file in self.cache_dir.glob("*.parquet"):
            try:
                file_time = datetime.fromtimestamp(cache_file.stat().st_mtime)
                if file_time < cutoff_time:
                    cache_file.unlink()
                    deleted_count += 1
                    logger.debug(f"åˆ é™¤è¿‡æœŸç¼“å­˜: {cache_file.name}")
            except Exception as e:
                logger.warning(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file.name}: {e}")

        logger.info(f"=== ç¼“å­˜æ¸…ç†å®Œæˆ: åˆ é™¤äº† {deleted_count} ä¸ªè¿‡æœŸæ–‡ä»¶ ===")
    
    async def run_full_update(self):
        """æ‰§è¡Œå®Œæ•´çš„å¤œé—´æ›´æ–°æµç¨‹"""
        start_time = datetime.now()
        logger.info(f"ğŸŒ™ å¤œé—´ç¼“å­˜æ›´æ–°å¼€å§‹: {start_time}")
        
        try:
            # 1. æ¯æ—¥æ›´æ–°
            daily_success, daily_total = await self.run_daily_updates()
            
            # 2. æ¯å‘¨æ›´æ–°
            weekly_success, weekly_total = await self.run_weekly_updates()
            
            # 3. æ¸…ç†è¿‡æœŸç¼“å­˜
            await self.cleanup_old_cache()
            
            # 4. æ€»ç»“
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info(f"ğŸ‰ å¤œé—´æ›´æ–°å®Œæˆ!")
            logger.info(f"   è€—æ—¶: {duration}")
            logger.info(f"   æ¯æ—¥æ›´æ–°: {daily_success}/{daily_total}")
            logger.info(f"   æ¯å‘¨æ›´æ–°: {weekly_success}/{weekly_total}")
            
            return True
            
        except Exception as e:
            logger.error(f"ğŸ’¥ å¤œé—´æ›´æ–°å¤±è´¥: {str(e)}", exc_info=True)
            return False

async def main():
    """ä¸»å‡½æ•°"""
    updater = NightlyCacheUpdater()
    success = await updater.run_full_update()
    
    # é€€å‡ºç ï¼š0è¡¨ç¤ºæˆåŠŸï¼Œ1è¡¨ç¤ºå¤±è´¥
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    asyncio.run(main())
