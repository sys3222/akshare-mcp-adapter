#!/usr/bin/env python3
"""
ç»¼åˆæµ‹è¯•æ‰§è¡Œè„šæœ¬
ä½¿ç”¨llm_test_cases.jsonä¸­çš„æµ‹è¯•ç”¨ä¾‹è¿›è¡Œå…¨é¢æµ‹è¯•
"""

import sys
import json
import asyncio
import time
import random
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from handlers.llm_handler import LLMAnalysisHandler
from models.schemas import IntentType, AnalysisContext, AnalysisResult

class ComprehensiveTestRunner:
    """ç»¼åˆæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, config_file: str = "llm_test_cases.json"):
        self.config_file = config_file
        self.config = self.load_config()
        self.llm_handler = LLMAnalysisHandler(use_llm=False)
        self.test_results = []
        self.start_time = None
        
    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•é…ç½®"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ç»¼åˆæµ‹è¯•æ‰§è¡Œ")
        print("=" * 80)
        
        self.start_time = time.time()
        
        # è¿è¡ŒåŸºç¡€æµ‹è¯•ç”¨ä¾‹
        await self.run_basic_test_cases()
        
        # è¿è¡ŒLLMç‰¹å®šæµ‹è¯•
        await self.run_llm_specific_tests()
        
        # è¿è¡ŒåŠ¨æ€ç”Ÿæˆçš„æµ‹è¯•
        await self.run_dynamic_tests()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_comprehensive_report()
    
    async def run_basic_test_cases(self):
        """è¿è¡ŒåŸºç¡€æµ‹è¯•ç”¨ä¾‹"""
        print("\nğŸ“‹ è¿è¡ŒåŸºç¡€æµ‹è¯•ç”¨ä¾‹...")
        
        test_cases = self.config.get('test_cases', [])
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"ğŸ§ª [{i}/{len(test_cases)}] {test_case['name']}")
            
            try:
                result = await self.execute_test_case(test_case)
                self.test_results.append(result)
                
                # æ˜¾ç¤ºç®€è¦ç»“æœ
                status = "âœ…" if result['success'] else "âŒ"
                score = result.get('score', 0)
                print(f"   {status} å¾—åˆ†: {score:.1f}/10")
                
                # å»¶è¿Ÿé¿å…è¿‡è½½
                if self.config.get('test_configuration', {}).get('delay_between_tests', 0) > 0:
                    await asyncio.sleep(self.config['test_configuration']['delay_between_tests'])
                    
            except Exception as e:
                print(f"   âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
                self.test_results.append({
                    'test_id': test_case['id'],
                    'success': False,
                    'error': str(e),
                    'score': 0
                })
    
    async def run_llm_specific_tests(self):
        """è¿è¡ŒLLMç‰¹å®šæµ‹è¯•"""
        print("\nğŸ§  è¿è¡ŒLLMç‰¹å®šæµ‹è¯•...")
        
        llm_tests = self.config.get('llm_specific_tests', [])
        
        for i, test_case in enumerate(llm_tests, 1):
            print(f"ğŸ”¬ [{i}/{len(llm_tests)}] {test_case['name']}")
            
            try:
                result = await self.execute_llm_test(test_case)
                self.test_results.append(result)
                
                status = "âœ…" if result['success'] else "âŒ"
                print(f"   {status} {result.get('summary', 'å®Œæˆ')}")
                
            except Exception as e:
                print(f"   âŒ LLMæµ‹è¯•å¤±è´¥: {e}")
    
    async def run_dynamic_tests(self):
        """è¿è¡ŒåŠ¨æ€ç”Ÿæˆçš„æµ‹è¯•"""
        print("\nğŸ² è¿è¡ŒåŠ¨æ€ç”Ÿæˆæµ‹è¯•...")
        
        test_data = self.config.get('test_data', {})
        stock_codes = test_data.get('sample_stock_codes', [])
        queries = test_data.get('sample_queries', [])
        
        # ç”Ÿæˆéšæœºæµ‹è¯•ç»„åˆ
        for i in range(10):  # ç”Ÿæˆ10ä¸ªéšæœºæµ‹è¯•
            stock_code = random.choice(stock_codes)
            query_template = random.choice(queries)
            query = query_template.format(stock_code=stock_code)
            
            print(f"ğŸ¯ [åŠ¨æ€{i+1}] {query}")
            
            try:
                result = await self.llm_handler.analyze_query(query, "test_user")
                
                # ç®€å•è¯„ä¼°
                score = self.evaluate_dynamic_result(result, query)
                
                self.test_results.append({
                    'test_id': f'dynamic_{i+1}',
                    'query': query,
                    'success': True,
                    'score': score,
                    'type': 'dynamic'
                })
                
                print(f"   âœ… å¾—åˆ†: {score:.1f}/10")
                
            except Exception as e:
                print(f"   âŒ åŠ¨æ€æµ‹è¯•å¤±è´¥: {e}")
    
    async def execute_test_case(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        start_time = time.time()
        
        try:
            # æ‰§è¡ŒæŸ¥è¯¢
            result = await self.llm_handler.analyze_query(test_case['prompt'], "test_user")
            execution_time = time.time() - start_time
            
            # è¯„ä¼°ç»“æœ
            score = self.evaluate_result(test_case, result, execution_time)
            
            return {
                'test_id': test_case['id'],
                'name': test_case['name'],
                'category': test_case.get('category', 'unknown'),
                'priority': test_case.get('priority', 'medium'),
                'query': test_case['prompt'],
                'success': True,
                'score': score,
                'execution_time': execution_time,
                'result_summary': result.summary,
                'insights_count': len(result.insights),
                'recommendations_count': len(result.recommendations),
                'confidence': result.confidence,
                'risk_level': result.risk_level
            }
            
        except Exception as e:
            return {
                'test_id': test_case['id'],
                'name': test_case['name'],
                'success': False,
                'error': str(e),
                'score': 0,
                'execution_time': time.time() - start_time
            }
    
    async def execute_llm_test(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡ŒLLMç‰¹å®šæµ‹è¯•"""
        try:
            result = await self.llm_handler.analyze_query(test_case['prompt'], "test_user")
            
            # æ ¹æ®æµ‹è¯•ç±»å‹è¿›è¡Œç‰¹å®šéªŒè¯
            validation_result = self.validate_llm_test(test_case, result)
            
            return {
                'test_id': test_case['id'],
                'name': test_case['name'],
                'test_type': test_case.get('test_type', 'unknown'),
                'success': validation_result['success'],
                'summary': validation_result['summary'],
                'details': validation_result.get('details', {})
            }
            
        except Exception as e:
            return {
                'test_id': test_case['id'],
                'success': False,
                'error': str(e)
            }
    
    def evaluate_result(self, test_case: Dict[str, Any], result: AnalysisResult, execution_time: float) -> float:
        """è¯„ä¼°æµ‹è¯•ç»“æœ"""
        score = 0.0
        max_score = 10.0
        
        # 1. åŸºç¡€å®Œæ•´æ€§ (3åˆ†)
        if result.summary and len(result.summary) > 10:
            score += 1.0
        if len(result.insights) > 0:
            score += 1.0
        if len(result.recommendations) > 0:
            score += 1.0
        
        # 2. ç½®ä¿¡åº¦åˆç†æ€§ (2åˆ†)
        if 0.3 <= result.confidence <= 1.0:
            score += 2.0
        elif 0.0 <= result.confidence < 0.3:
            score += 1.0
        
        # 3. å“åº”æ—¶é—´ (2åˆ†)
        if execution_time < 3.0:
            score += 2.0
        elif execution_time < 10.0:
            score += 1.0
        
        # 4. å†…å®¹è´¨é‡ (2åˆ†)
        content_quality = self.assess_content_quality(result, test_case)
        score += content_quality
        
        # 5. é£é™©è¯„ä¼° (1åˆ†)
        if result.risk_level in ["ä½é£é™©", "ä¸­ç­‰é£é™©", "é«˜é£é™©", "æœªçŸ¥"]:
            score += 1.0
        
        return min(score, max_score)
    
    def assess_content_quality(self, result: AnalysisResult, test_case: Dict[str, Any]) -> float:
        """è¯„ä¼°å†…å®¹è´¨é‡"""
        quality_score = 0.0
        
        # æ£€æŸ¥å…³é”®è¯è¦†ç›–
        expected_keywords = test_case.get('expected_entities', [])
        content_text = f"{result.summary} {' '.join(result.insights)} {' '.join(result.recommendations)}".lower()
        
        if expected_keywords:
            keyword_coverage = sum(1 for keyword in expected_keywords 
                                 if keyword.lower() in content_text) / len(expected_keywords)
            quality_score += keyword_coverage * 1.0
        else:
            quality_score += 1.0
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦å’Œè¯¦ç»†ç¨‹åº¦
        total_content_length = len(result.summary) + sum(len(insight) for insight in result.insights)
        if total_content_length > 100:
            quality_score += 1.0
        elif total_content_length > 50:
            quality_score += 0.5
        
        return min(quality_score, 2.0)
    
    def evaluate_dynamic_result(self, result: AnalysisResult, query: str) -> float:
        """è¯„ä¼°åŠ¨æ€æµ‹è¯•ç»“æœ"""
        score = 0.0
        
        # åŸºç¡€æ£€æŸ¥
        if result.summary:
            score += 3.0
        if len(result.insights) > 0:
            score += 2.0
        if len(result.recommendations) > 0:
            score += 2.0
        if result.confidence > 0.0:
            score += 2.0
        if result.risk_level != "æœªçŸ¥":
            score += 1.0
        
        return min(score, 10.0)
    
    def validate_llm_test(self, test_case: Dict[str, Any], result: AnalysisResult) -> Dict[str, Any]:
        """éªŒè¯LLMç‰¹å®šæµ‹è¯•"""
        test_type = test_case.get('test_type', 'unknown')
        
        if test_type == 'tool_calling':
            return self.validate_tool_calling(test_case, result)
        elif test_type == 'data_analysis':
            return self.validate_data_analysis(test_case, result)
        elif test_type == 'reasoning':
            return self.validate_reasoning(test_case, result)
        elif test_type == 'error_handling':
            return self.validate_error_handling(test_case, result)
        else:
            return {'success': True, 'summary': 'åŸºç¡€éªŒè¯é€šè¿‡'}
    
    def validate_tool_calling(self, test_case: Dict[str, Any], result: AnalysisResult) -> Dict[str, Any]:
        """éªŒè¯å·¥å…·è°ƒç”¨"""
        # æ›´å…¨é¢çš„å·¥å…·è°ƒç”¨æ£€æµ‹
        indicators = []

        # æ£€æŸ¥æ•°æ®ç‚¹
        if len(result.data_points) > 0:
            indicators.append("æœ‰æ•°æ®ç‚¹")

        # æ£€æŸ¥æ´å¯Ÿæ•°é‡
        if len(result.insights) > 0:
            indicators.append("æœ‰åˆ†ææ´å¯Ÿ")

        # æ£€æŸ¥å»ºè®®æ•°é‡
        if len(result.recommendations) > 0:
            indicators.append("æœ‰æŠ•èµ„å»ºè®®")

        # æ£€æŸ¥ç½®ä¿¡åº¦
        if result.confidence > 0.5:
            indicators.append("ç½®ä¿¡åº¦è¾ƒé«˜")

        # æ£€æŸ¥æ‘˜è¦å†…å®¹
        if result.summary and len(result.summary) > 20:
            indicators.append("æœ‰è¯¦ç»†æ‘˜è¦")

        # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“çš„è‚¡ç¥¨ä»£ç è¯†åˆ«
        if any(code in result.summary for code in ['000001', '600519', '000002']):
            indicators.append("è¯†åˆ«åˆ°è‚¡ç¥¨ä»£ç ")

        success = len(indicators) >= 3  # è‡³å°‘æ»¡è¶³3ä¸ªæŒ‡æ ‡

        return {
            'success': success,
            'summary': f'å·¥å…·è°ƒç”¨éªŒè¯{"é€šè¿‡" if success else "å¤±è´¥"} - æ»¡è¶³{len(indicators)}ä¸ªæŒ‡æ ‡',
            'details': {'indicators': indicators}
        }
    
    def validate_data_analysis(self, test_case: Dict[str, Any], result: AnalysisResult) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®åˆ†æ"""
        analysis_quality = []

        # æ£€æŸ¥æ´å¯Ÿè´¨é‡
        if len(result.insights) >= 2:
            analysis_quality.append("æ´å¯Ÿæ•°é‡å……è¶³")
        elif len(result.insights) >= 1:
            analysis_quality.append("æœ‰åŸºç¡€æ´å¯Ÿ")

        # æ£€æŸ¥æ•°æ®ç‚¹
        if len(result.data_points) > 0:
            analysis_quality.append("æœ‰æ•°æ®æ”¯æ’‘")

        # æ£€æŸ¥åˆ†ææ·±åº¦
        total_content = len(result.summary) + sum(len(insight) for insight in result.insights)
        if total_content > 200:
            analysis_quality.append("åˆ†æå†…å®¹è¯¦ç»†")
        elif total_content > 100:
            analysis_quality.append("åˆ†æå†…å®¹é€‚ä¸­")

        # æ£€æŸ¥ä¸“ä¸šæ€§
        professional_terms = ['PE', 'PB', 'ROE', 'å¸‚ç›ˆç‡', 'ä¼°å€¼', 'åŸºæœ¬é¢', 'æŠ€æœ¯é¢', 'é£é™©']
        content_text = f"{result.summary} {' '.join(result.insights)}".lower()
        if any(term.lower() in content_text for term in professional_terms):
            analysis_quality.append("åŒ…å«ä¸“ä¸šæœ¯è¯­")

        success = len(analysis_quality) >= 2

        return {
            'success': success,
            'summary': f'æ•°æ®åˆ†æéªŒè¯{"é€šè¿‡" if success else "ä¸å……åˆ†"} - {len(analysis_quality)}ä¸ªè´¨é‡æŒ‡æ ‡',
            'details': {'quality_indicators': analysis_quality}
        }
    
    def validate_reasoning(self, test_case: Dict[str, Any], result: AnalysisResult) -> Dict[str, Any]:
        """éªŒè¯æ¨ç†èƒ½åŠ›"""
        has_reasoning = len(result.recommendations) > 0 and result.confidence > 0.5
        
        return {
            'success': has_reasoning,
            'summary': 'æ¨ç†éªŒè¯é€šè¿‡' if has_reasoning else 'æ¨ç†èƒ½åŠ›ä¸è¶³'
        }
    
    def validate_error_handling(self, test_case: Dict[str, Any], result: AnalysisResult) -> Dict[str, Any]:
        """éªŒè¯é”™è¯¯å¤„ç†"""
        # é”™è¯¯å¤„ç†æµ‹è¯•åº”è¯¥æœ‰åˆç†çš„å“åº”ï¼Œå³ä½¿æ•°æ®è·å–å¤±è´¥
        has_graceful_handling = result.summary and "é”™è¯¯" not in result.summary.lower()
        
        return {
            'success': has_graceful_handling,
            'summary': 'é”™è¯¯å¤„ç†éªŒè¯é€šè¿‡' if has_graceful_handling else 'é”™è¯¯å¤„ç†éœ€è¦æ”¹è¿›'
        }
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        total_time = time.time() - self.start_time
        
        print("\n" + "=" * 80)
        print("ğŸ“Š ç»¼åˆæµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        
        # æ€»ä½“ç»Ÿè®¡
        total_tests = len(self.test_results)
        successful_tests = sum(1 for r in self.test_results if r.get('success', False))
        average_score = sum(r.get('score', 0) for r in self.test_results) / total_tests if total_tests > 0 else 0
        
        print(f"ğŸ“ˆ æ€»ä½“è¡¨ç°:")
        print(f"   æµ‹è¯•ç”¨ä¾‹æ€»æ•°: {total_tests}")
        print(f"   æˆåŠŸæ‰§è¡Œ: {successful_tests}/{total_tests} ({successful_tests/total_tests*100:.1f}%)")
        print(f"   å¹³å‡å¾—åˆ†: {average_score:.1f}/10")
        print(f"   æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ç§’")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        self.generate_category_stats()
        
        # æŒ‰ä¼˜å…ˆçº§ç»Ÿè®¡
        self.generate_priority_stats()
        
        # æ€§èƒ½åˆ†æ
        self.generate_performance_analysis()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        self.save_detailed_report()
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        self.generate_improvement_suggestions()
    
    def generate_category_stats(self):
        """ç”Ÿæˆåˆ†ç±»ç»Ÿè®¡"""
        print(f"\nğŸ“‹ åˆ†ç±»è¡¨ç°:")
        
        categories = {}
        for result in self.test_results:
            cat = result.get('category', 'unknown')
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(result)
        
        for category, results in categories.items():
            cat_score = sum(r.get('score', 0) for r in results) / len(results)
            cat_success = sum(1 for r in results if r.get('success', False)) / len(results) * 100
            print(f"   {category}: {cat_score:.1f}/10 (æˆåŠŸç‡: {cat_success:.1f}%)")
    
    def generate_priority_stats(self):
        """ç”Ÿæˆä¼˜å…ˆçº§ç»Ÿè®¡"""
        print(f"\nğŸ¯ ä¼˜å…ˆçº§è¡¨ç°:")
        
        priorities = {}
        for result in self.test_results:
            pri = result.get('priority', 'medium')
            if pri not in priorities:
                priorities[pri] = []
            priorities[pri].append(result)
        
        for priority in ['high', 'medium', 'low']:
            if priority in priorities:
                results = priorities[priority]
                pri_score = sum(r.get('score', 0) for r in results) / len(results)
                pri_success = sum(1 for r in results if r.get('success', False)) / len(results) * 100
                print(f"   {priority}: {pri_score:.1f}/10 (æˆåŠŸç‡: {pri_success:.1f}%)")
    
    def generate_performance_analysis(self):
        """ç”Ÿæˆæ€§èƒ½åˆ†æ"""
        print(f"\nâš¡ æ€§èƒ½åˆ†æ:")
        
        execution_times = [r.get('execution_time', 0) for r in self.test_results if r.get('execution_time')]
        if execution_times:
            avg_time = sum(execution_times) / len(execution_times)
            max_time = max(execution_times)
            min_time = min(execution_times)
            
            print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")
            print(f"   æœ€å¿«å“åº”: {min_time:.2f}ç§’")
            print(f"   æœ€æ…¢å“åº”: {max_time:.2f}ç§’")
    
    def generate_improvement_suggestions(self):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        
        # åˆ†æå¤±è´¥çš„æµ‹è¯•
        failed_tests = [r for r in self.test_results if not r.get('success', False)]
        if failed_tests:
            print(f"   ğŸ”§ {len(failed_tests)} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤é”™è¯¯å¤„ç†")
        
        # åˆ†æä½åˆ†æµ‹è¯•
        low_score_tests = [r for r in self.test_results if r.get('score', 0) < 5.0]
        if low_score_tests:
            print(f"   ğŸ“ˆ {len(low_score_tests)} ä¸ªæµ‹è¯•å¾—åˆ†è¾ƒä½ï¼Œéœ€è¦æ”¹è¿›ç®—æ³•")
        
        # åˆ†ææ€§èƒ½é—®é¢˜
        slow_tests = [r for r in self.test_results if r.get('execution_time', 0) > 10.0]
        if slow_tests:
            print(f"   âš¡ {len(slow_tests)} ä¸ªæµ‹è¯•å“åº”è¾ƒæ…¢ï¼Œéœ€è¦ä¼˜åŒ–æ€§èƒ½")
    
    def save_detailed_report(self):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Š"""
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_tests": len(self.test_results),
                "successful_tests": sum(1 for r in self.test_results if r.get('success', False)),
                "average_score": sum(r.get('score', 0) for r in self.test_results) / len(self.test_results),
                "total_execution_time": time.time() - self.start_time
            },
            "detailed_results": self.test_results,
            "configuration": self.config
        }
        
        with open("comprehensive_test_report.json", "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: comprehensive_test_report.json")

async def main():
    """ä¸»å‡½æ•°"""
    runner = ComprehensiveTestRunner()
    await runner.run_all_tests()

if __name__ == "__main__":
    asyncio.run(main())
