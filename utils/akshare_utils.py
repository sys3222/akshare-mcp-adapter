import os
import pandas as pd
import akshare as ak
import backtrader as bt
from datetime import datetime
import logging

logger = logging.getLogger("mcp-unified-service")

def init_akshare_cache():
    """
    Initialize AkShare cache directories and preload common data
    """
    # Create cache directories
    os.makedirs('etf_cache', exist_ok=True)
    os.makedirs('index_cache', exist_ok=True)
    os.makedirs('stock_cache', exist_ok=True)
    
    logger.info("åˆå§‹åŒ–AkShareç¼“å­˜ç›®å½•")
    
    # Preload common indices (optional)
    try:
        # é¢„åŠ è½½æ²ªæ·±300æŒ‡æ•°ä½œä¸ºå¸¸ç”¨åŸºå‡†
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = '2020-01-01'  # é»˜è®¤åŠ è½½è¿‘3å¹´æ•°æ®
        
        logger.info(f"é¢„åŠ è½½æ²ªæ·±300æŒ‡æ•°æ•°æ®: {start_date} ~ {end_date}")
        get_index_nav('000300', start_date, end_date, force_update=True)
        logger.info("æ²ªæ·±300æŒ‡æ•°æ•°æ®åŠ è½½å®Œæˆ")
    except Exception as e:
        logger.warning(f"é¢„åŠ è½½æŒ‡æ•°æ•°æ®å¤±è´¥: {str(e)}")

def get_akshare_etf_data(symbol, start, end, cache_dir='etf_cache', force_update=False):
    """
    Fetch ETF data from AkShare with caching support
    
    Args:
        symbol: ETF symbol (e.g., '518880')
        start: Start date in 'YYYY-MM-DD' format
        end: End date in 'YYYY-MM-DD' format
        cache_dir: Directory to store cached data
        force_update: Whether to force update the cache
        
    Returns:
        bt.feeds.PandasData: Backtrader data feed
    """
    os.makedirs(cache_dir, exist_ok=True)
    cache_file = os.path.join(cache_dir, f'{symbol}.pkl')
    start_dt, end_dt = pd.to_datetime(start), pd.to_datetime(end)
    need_download = force_update or (not os.path.exists(cache_file))

    if not need_download:
        try:
            df = pd.read_pickle(cache_file)
            df['date'] = pd.to_datetime(df['æ—¥æœŸ'])
            # Check if cache covers the required time range
            if df['date'].min() > start_dt or df['date'].max() < end_dt:
                print(f"ðŸ“Œ Cache time range incomplete: {df['date'].min().date()} ~ {df['date'].max().date()}, need to download again")
                need_download = True
        except Exception as e:
            print(f"âš ï¸ Failed to read cache: {e}")
            need_download = True

    if need_download:
        print(f"â¬‡ï¸ Downloading ETF data: {symbol}")
        df = ak.fund_etf_hist_em(symbol=symbol)
        if df.empty:
            raise ValueError(f"No data returned from AkShare for symbol {symbol}")
        df['date'] = pd.to_datetime(df['æ—¥æœŸ'])
        df.to_pickle(cache_file)

    # Filter by date range
    df = df[(df['date'] >= start_dt) & (df['date'] <= end_dt)]
    if df.empty:
        raise ValueError(f"âŒ No data available in the specified date range: {start} ~ {end}")

    # Format for backtrader
    df.rename(columns={'date': 'datetime',
                       'å¼€ç›˜': 'open',
                       'æœ€é«˜': 'high',
                       'æœ€ä½Ž': 'low',
                       'æ”¶ç›˜': 'close',
                       'æˆäº¤é‡': 'volume'}, inplace=True)
    df = df[['datetime', 'open', 'high', 'low', 'close', 'volume']]
    df['openinterest'] = 0

    data = bt.feeds.PandasData(
        dataname=df,
        datetime='datetime',
        open='open',
        high='high',
        low='low',
        close='close',
        volume='volume',
        openinterest='openinterest',
        name=symbol
    )

    return data

def get_index_nav(symbol, start, end, cache_dir='index_cache', force_update=False):
    """
    Fetch index data from AkShare with caching support
    
    Args:
        symbol: Index symbol (e.g., '000300' for CSI 300)
        start: Start date in 'YYYY-MM-DD' format
        end: End date in 'YYYY-MM-DD' format
        cache_dir: Directory to store cached data
        force_update: Whether to force update the cache
        
    Returns:
        pd.Series: Index NAV series
    """
    os.makedirs(cache_dir, exist_ok=True)
    cache_file = os.path.join(cache_dir, f'{symbol}.pkl')
    start_dt, end_dt = pd.to_datetime(start), pd.to_datetime(end)
    need_download = force_update or (not os.path.exists(cache_file))

    if not need_download:
        try:
            df = pd.read_pickle(cache_file)
            df['date'] = pd.to_datetime(df['æ—¥æœŸ'])
            # Check if cache covers the required time range
            if df['date'].min() > start_dt or df['date'].max() < end_dt:
                print(f"ðŸ“Œ Cache time range incomplete: {df['date'].min().date()} ~ {df['date'].max().date()}, need to download again")
                need_download = True
        except Exception as e:
            print(f"âš ï¸ Failed to read cache: {e}")
            need_download = True

    if need_download:
        print(f"â¬‡ï¸ Downloading index data: {symbol}")
        df = ak.index_zh_a_hist(symbol=symbol, period='daily')
        if df.empty:
            raise ValueError(f"No data returned from AkShare for index {symbol}")
        df['date'] = pd.to_datetime(df['æ—¥æœŸ'])
        df.to_pickle(cache_file)

    # Filter by date range
    df = df[(df['date'] >= start_dt) & (df['date'] <= end_dt)]
    if df.empty:
        raise ValueError(f"âŒ No data available in the specified date range: {start} ~ {end}")

    # Calculate NAV
    df = df.sort_values('date')
    df['nav'] = df['æ”¶ç›˜'] / df['æ”¶ç›˜'].iloc[0]
    
    return df.set_index('date')['nav']

def get_stock_data(symbol, start, end, cache_dir='stock_cache', force_update=False):
    """
    Fetch stock data from AkShare with caching support
    
    Args:
        symbol: Stock symbol (e.g., '000001')
        start: Start date in 'YYYY-MM-DD' format
        end: End date in 'YYYY-MM-DD' format
        cache_dir: Directory to store cached data
        force_update: Whether to force update the cache
        
    Returns:
        bt.feeds.PandasData: Backtrader data feed
    """
    os.makedirs(cache_dir, exist_ok=True)
    cache_file = os.path.join(cache_dir, f'{symbol}.pkl')
    start_dt, end_dt = pd.to_datetime(start), pd.to_datetime(end)
    need_download = force_update or (not os.path.exists(cache_file))

    if not need_download:
        try:
            df = pd.read_pickle(cache_file)
            df['date'] = pd.to_datetime(df['æ—¥æœŸ'])
            # Check if cache covers the required time range
            if df['date'].min() > start_dt or df['date'].max() < end_dt:
                logger.info(f"ç¼“å­˜æ—¶é—´èŒƒå›´ä¸å®Œæ•´: {df['date'].min().date()} ~ {df['date'].max().date()}, éœ€è¦é‡æ–°ä¸‹è½½")
                need_download = True
        except Exception as e:
            logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            need_download = True

    if need_download:
        logger.info(f"ä¸‹è½½è‚¡ç¥¨æ•°æ®: {symbol}")
        try:
            # ä½¿ç”¨ä¸œæ–¹è´¢å¯Œç½‘æ•°æ®æº
            df = ak.stock_zh_a_hist(symbol=symbol, period="daily", 
                                    start_date=start.replace('-', ''), 
                                    end_date=end.replace('-', ''), 
                                    adjust="qfq")
            if df.empty:
                raise ValueError(f"AkShareæœªè¿”å›žè‚¡ç¥¨{symbol}çš„æ•°æ®")
            df['date'] = pd.to_datetime(df['æ—¥æœŸ'])
            df.to_pickle(cache_file)
        except Exception as e:
            logger.error(f"ä¸‹è½½è‚¡ç¥¨æ•°æ®å¤±è´¥: {str(e)}")
            raise

    # Filter by date range
    df = df[(df['date'] >= start_dt) & (df['date'] <= end_dt)]
    if df.empty:
        raise ValueError(f"æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ²¡æœ‰æ•°æ®: {start} ~ {end}")

    # Format for backtrader
    df.rename(columns={'date': 'datetime',
                       'å¼€ç›˜': 'open',
                       'æœ€é«˜': 'high',
                       'æœ€ä½Ž': 'low',
                       'æ”¶ç›˜': 'close',
                       'æˆäº¤é‡': 'volume'}, inplace=True)
    df = df[['datetime', 'open', 'high', 'low', 'close', 'volume']]
    df['openinterest'] = 0

    data = bt.feeds.PandasData(
        dataname=df,
        datetime='datetime',
        open='open',
        high='high',
        low='low',
        close='close',
        volume='volume',
        openinterest='openinterest',
        name=symbol
    )

    return data