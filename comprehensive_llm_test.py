#!/usr/bin/env python3
"""
LLMç³»ç»Ÿç»¼åˆæµ‹è¯•æ¡†æ¶
ç³»ç»ŸåŒ–æµ‹è¯•LLMå›ç­”æ•ˆæœå’Œè´¨é‡
"""

import sys
import asyncio
import json
import time
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from handlers.llm_handler import LLMAnalysisHandler
from models.schemas import IntentType, AnalysisContext, AnalysisResult

class LLMTestFramework:
    """LLMæµ‹è¯•æ¡†æ¶"""
    
    def __init__(self):
        self.llm_handler = LLMAnalysisHandler(use_llm=False)  # å…ˆæµ‹è¯•è§„åˆ™æ¨¡å¼
        self.test_results = []
        self.performance_metrics = {}
        
    def create_test_cases(self) -> List[Dict[str, Any]]:
        """åˆ›å»ºæµ‹è¯•ç”¨ä¾‹"""
        return [
            # åŸºç¡€è‚¡ç¥¨åˆ†ææµ‹è¯•
            {
                "id": "stock_basic_001",
                "category": "è‚¡ç¥¨åˆ†æ",
                "query": "åˆ†æ000001å¹³å®‰é“¶è¡Œ",
                "expected_intent": IntentType.STOCK_ANALYSIS,
                "expected_keywords": ["å¹³å®‰é“¶è¡Œ", "000001", "è‚¡ä»·", "åˆ†æ"],
                "quality_criteria": {
                    "min_insights": 1,
                    "min_recommendations": 1,
                    "min_confidence": 0.7
                }
            },
            {
                "id": "stock_basic_002", 
                "category": "è‚¡ç¥¨åˆ†æ",
                "query": "000001æ€ä¹ˆæ ·ï¼Ÿå€¼å¾—æŠ•èµ„å—ï¼Ÿ",
                "expected_intent": IntentType.STOCK_ANALYSIS,
                "expected_keywords": ["æŠ•èµ„", "å»ºè®®", "é£é™©"],
                "quality_criteria": {
                    "min_insights": 1,
                    "min_recommendations": 2,
                    "min_confidence": 0.6
                }
            },
            
            # å¸‚åœºæ¦‚è§ˆæµ‹è¯•
            {
                "id": "market_001",
                "category": "å¸‚åœºæ¦‚è§ˆ", 
                "query": "ä»Šæ—¥Aè‚¡å¸‚åœºè¡¨ç°å¦‚ä½•ï¼Ÿ",
                "expected_intent": IntentType.MARKET_OVERVIEW,
                "expected_keywords": ["Aè‚¡", "å¸‚åœº", "è¡¨ç°"],
                "quality_criteria": {
                    "min_insights": 1,
                    "min_confidence": 0.5
                }
            },
            {
                "id": "market_002",
                "category": "å¸‚åœºæ¦‚è§ˆ",
                "query": "æ•´ä½“å¸‚åœºèµ°åŠ¿æ€ä¹ˆæ ·ï¼Ÿ",
                "expected_intent": IntentType.MARKET_OVERVIEW,
                "expected_keywords": ["å¸‚åœº", "èµ°åŠ¿", "æ•´ä½“"],
                "quality_criteria": {
                    "min_insights": 1,
                    "min_confidence": 0.5
                }
            },
            
            # è´¢åŠ¡æŒ‡æ ‡æµ‹è¯•
            {
                "id": "financial_001",
                "category": "è´¢åŠ¡æŒ‡æ ‡",
                "query": "000001çš„PEå’ŒROEæ€ä¹ˆæ ·ï¼Ÿ",
                "expected_intent": IntentType.FINANCIAL_METRICS,
                "expected_keywords": ["PE", "ROE", "è´¢åŠ¡", "æŒ‡æ ‡"],
                "quality_criteria": {
                    "min_insights": 1,
                    "min_confidence": 0.6
                }
            },
            
            # å¤æ‚æŸ¥è¯¢æµ‹è¯•
            {
                "id": "complex_001",
                "category": "å¤æ‚æŸ¥è¯¢",
                "query": "å¸®æˆ‘å¯¹æ¯”åˆ†æ000001å’Œ600519ï¼Œå“ªä¸ªæ›´å€¼å¾—æŠ•èµ„ï¼Ÿ",
                "expected_intent": IntentType.COMPARISON_ANALYSIS,
                "expected_keywords": ["å¯¹æ¯”", "000001", "600519", "æŠ•èµ„"],
                "quality_criteria": {
                    "min_insights": 2,
                    "min_recommendations": 2,
                    "min_confidence": 0.7
                }
            },
            
            # è¾¹ç•Œæƒ…å†µæµ‹è¯•
            {
                "id": "edge_001",
                "category": "è¾¹ç•Œæƒ…å†µ",
                "query": "éšæœºæ–‡æœ¬æµ‹è¯•",
                "expected_intent": IntentType.UNKNOWN,
                "expected_keywords": [],
                "quality_criteria": {
                    "min_confidence": 0.0,
                    "max_confidence": 0.5
                }
            },
            {
                "id": "edge_002",
                "category": "è¾¹ç•Œæƒ…å†µ", 
                "query": "",
                "expected_intent": IntentType.UNKNOWN,
                "expected_keywords": [],
                "quality_criteria": {
                    "min_confidence": 0.0,
                    "max_confidence": 0.3
                }
            }
        ]
    
    async def run_single_test(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        print(f"ğŸ§ª æµ‹è¯• {test_case['id']}: {test_case['query'][:50]}...")
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œåˆ†æ
            result = await self.llm_handler.analyze_query(test_case['query'], "test_user")
            
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            execution_time = time.time() - start_time
            
            # è¯„ä¼°ç»“æœ
            evaluation = self.evaluate_result(test_case, result, execution_time)
            
            print(f"   âœ… å®Œæˆ - å¾—åˆ†: {evaluation['score']:.1f}/10")
            return evaluation
            
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {str(e)}")
            return {
                "test_id": test_case['id'],
                "category": test_case['category'],
                "query": test_case['query'],
                "success": False,
                "error": str(e),
                "score": 0,
                "execution_time": time.time() - start_time
            }
    
    def evaluate_result(self, test_case: Dict[str, Any], result: AnalysisResult, execution_time: float) -> Dict[str, Any]:
        """è¯„ä¼°æµ‹è¯•ç»“æœ"""
        evaluation = {
            "test_id": test_case['id'],
            "category": test_case['category'],
            "query": test_case['query'],
            "success": True,
            "execution_time": execution_time,
            "result_summary": result.summary,
            "insights_count": len(result.insights),
            "recommendations_count": len(result.recommendations),
            "confidence": result.confidence,
            "risk_level": result.risk_level
        }
        
        # è®¡ç®—è´¨é‡å¾—åˆ† (0-10åˆ†)
        score = 0
        max_score = 10
        issues = []
        
        # 1. æ„å›¾è¯†åˆ«å‡†ç¡®æ€§ (2åˆ†)
        context = self.llm_handler._identify_intent(test_case['query'])
        if context.intent == test_case['expected_intent']:
            score += 2
        else:
            issues.append(f"æ„å›¾è¯†åˆ«é”™è¯¯: æœŸæœ›{test_case['expected_intent']}, å®é™…{context.intent}")
        
        # 2. ç½®ä¿¡åº¦åˆç†æ€§ (2åˆ†)
        criteria = test_case['quality_criteria']
        if criteria.get('min_confidence', 0) <= result.confidence <= criteria.get('max_confidence', 1.0):
            score += 2
        else:
            issues.append(f"ç½®ä¿¡åº¦ä¸åˆç†: {result.confidence}")
        
        # 3. å†…å®¹è´¨é‡ (3åˆ†)
        if len(result.insights) >= criteria.get('min_insights', 0):
            score += 1.5
        else:
            issues.append(f"æ´å¯Ÿæ•°é‡ä¸è¶³: {len(result.insights)}")
            
        if len(result.recommendations) >= criteria.get('min_recommendations', 0):
            score += 1.5
        else:
            issues.append(f"å»ºè®®æ•°é‡ä¸è¶³: {len(result.recommendations)}")
        
        # 4. å…³é”®è¯è¦†ç›– (2åˆ†)
        content_text = f"{result.summary} {' '.join(result.insights)} {' '.join(result.recommendations)}".lower()
        keyword_coverage = sum(1 for keyword in test_case['expected_keywords'] 
                             if keyword.lower() in content_text)
        if test_case['expected_keywords']:
            keyword_score = (keyword_coverage / len(test_case['expected_keywords'])) * 2
            score += keyword_score
        else:
            score += 2  # æ— å…³é”®è¯è¦æ±‚æ—¶ç»™æ»¡åˆ†
        
        # 5. å“åº”æ—¶é—´ (1åˆ†)
        if execution_time < 5.0:
            score += 1
        elif execution_time < 10.0:
            score += 0.5
        else:
            issues.append(f"å“åº”æ—¶é—´è¿‡é•¿: {execution_time:.2f}s")
        
        evaluation.update({
            "score": min(score, max_score),
            "max_score": max_score,
            "issues": issues,
            "keyword_coverage": keyword_coverage if test_case['expected_keywords'] else "N/A"
        })
        
        return evaluation
    
    async def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹LLMç³»ç»Ÿç»¼åˆæµ‹è¯•")
        print("=" * 60)
        
        test_cases = self.create_test_cases()
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        for test_case in test_cases:
            result = await self.run_single_test(test_case)
            self.test_results.append(result)
            
            # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡è½½
            await asyncio.sleep(0.5)
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š LLMç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        # æ€»ä½“ç»Ÿè®¡
        total_tests = len(self.test_results)
        successful_tests = sum(1 for r in self.test_results if r['success'])
        average_score = sum(r.get('score', 0) for r in self.test_results) / total_tests if total_tests > 0 else 0
        average_time = sum(r['execution_time'] for r in self.test_results) / total_tests if total_tests > 0 else 0
        
        print(f"ğŸ“ˆ æ€»ä½“è¡¨ç°:")
        print(f"   æµ‹è¯•ç”¨ä¾‹æ€»æ•°: {total_tests}")
        print(f"   æˆåŠŸæ‰§è¡Œ: {successful_tests}/{total_tests} ({successful_tests/total_tests*100:.1f}%)")
        print(f"   å¹³å‡å¾—åˆ†: {average_score:.1f}/10")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {average_time:.2f}ç§’")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        print(f"\nğŸ“‹ åˆ†ç±»è¡¨ç°:")
        categories = {}
        for result in self.test_results:
            cat = result['category']
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(result)
        
        for category, results in categories.items():
            cat_score = sum(r.get('score', 0) for r in results) / len(results)
            cat_success = sum(1 for r in results if r['success']) / len(results) * 100
            print(f"   {category}: {cat_score:.1f}/10 (æˆåŠŸç‡: {cat_success:.1f}%)")
        
        # è¯¦ç»†ç»“æœ
        print(f"\nğŸ” è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for result in self.test_results:
            status = "âœ…" if result['success'] else "âŒ"
            score_text = f"{result.get('score', 0):.1f}/10" if result['success'] else "å¤±è´¥"
            print(f"   {status} {result['test_id']}: {score_text}")
            
            if result['success'] and result.get('issues'):
                for issue in result['issues']:
                    print(f"      âš ï¸ {issue}")
        
        # é—®é¢˜æ€»ç»“
        all_issues = []
        for result in self.test_results:
            if result.get('issues'):
                all_issues.extend(result['issues'])
        
        if all_issues:
            print(f"\nâš ï¸ å‘ç°çš„ä¸»è¦é—®é¢˜:")
            issue_counts = {}
            for issue in all_issues:
                issue_type = issue.split(':')[0]
                issue_counts[issue_type] = issue_counts.get(issue_type, 0) + 1
            
            for issue_type, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True):
                print(f"   {issue_type}: {count}æ¬¡")
        
        # æ”¹è¿›å»ºè®®
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        if average_score < 7:
            print("   ğŸ”§ æ•´ä½“è¡¨ç°éœ€è¦æ”¹è¿›ï¼Œå»ºè®®ä¼˜åŒ–ç®—æ³•å’Œæ¨¡æ¿")
        if average_time > 5:
            print("   âš¡ å“åº”æ—¶é—´è¾ƒæ…¢ï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½")
        if successful_tests < total_tests:
            print("   ğŸ› ï¸ å­˜åœ¨æ‰§è¡Œå¤±è´¥çš„æµ‹è¯•ï¼Œéœ€è¦ä¿®å¤é”™è¯¯å¤„ç†")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        self.save_detailed_report()
    
    def save_detailed_report(self):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_tests": len(self.test_results),
                "successful_tests": sum(1 for r in self.test_results if r['success']),
                "average_score": sum(r.get('score', 0) for r in self.test_results) / len(self.test_results),
                "average_time": sum(r['execution_time'] for r in self.test_results) / len(self.test_results)
            },
            "detailed_results": self.test_results
        }
        
        with open("llm_test_report.json", "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: llm_test_report.json")

async def main():
    """ä¸»å‡½æ•°"""
    framework = LLMTestFramework()
    await framework.run_comprehensive_test()

if __name__ == "__main__":
    asyncio.run(main())
